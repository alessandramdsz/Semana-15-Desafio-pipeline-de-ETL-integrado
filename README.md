# üìä Semana 15: Desafio pipeline de ETL integrado

Este projeto tem como objetivo principal o tratamento colaborativo de pipeline de ETL integrado, bem como desenvolvimento de um dashboard para elucidar os dados.

O fluxo de trabalho envolve:
1.  **[Passo 1: Coleta de dados]**: Dados extra√≠dos do Kaggle, disponibilizado arquivo previamente tratado em aula do Bootcamp. Arquivo disponibilizado em files, kaggle_survey_2022_mulheres_dados.csv
2.  **[Passo 2: Processamento e tratamento]**: Realizado no Google Colab, envolvendo limpeza, transforma√ß√£o e an√°lise explorat√≥ria dos dados. 
3.  **[Passo 3: Visualiza√ß√£o]**: O dashboard interativo foi criado no Tableau para apresentar os insights de forma clara.

---

### üíª Notebook de An√°lise (Google Colab)

Acesse o c√≥digo-fonte da an√°lise, incluindo a limpeza de dados e a execu√ß√£o dos modelos, neste link.
Execu√ß√£o do Collab: https://colab.research.google.com/drive/16a1AEhFoueWkiVsCsQ-IwJJ4RlaTMUS_?usp=sharing

### üìà Dashboard no Tableau

Clique no link abaixo para acessar o dashboard interativo com as visualiza√ß√µes finais: 
https://public.tableau.com/app/profile/alessandra.machado2999/viz/MulheresemTech_BootcampWoMakersCode/MulheresemTech 

## üìÇ Estrutura de files nesse reposit√≥rio

Aqui est√£o os arquivos e pastas chave dentro deste reposit√≥rio e suas fun√ß√µes:

| Arquivo/Pasta | Descri√ß√£o |
| :--- | :--- |
| `notebooks/` | Cont√©m os notebooks Jupyter/Colab utilizados para an√°lise (`analise_vendas.ipynb`). |
| `data/` | Pasta que armazena os dados brutos e processados (`dados_brutos.csv`, `dados_processados.csv`). |
| `src/` | (Opcional) Armazena scripts de c√≥digo-fonte auxiliares (ex: fun√ß√µes de limpeza de dados). |
| `README.md` | Este arquivo, com a descri√ß√£o e os links do projeto. |
| `requirements.txt` | Lista de bibliotecas Python necess√°rias para rodar o notebook. |

---

## üõ† Como Executar

Se voc√™ deseja replicar a an√°lise, siga os passos:

1.  Clone este reposit√≥rio: `git clone https://docs.github.com/pt/repositories/creating-and-managing-repositories/quickstart-for-repositories`
2.  Instale as depend√™ncias Python: `pip install -r requirements.txt`
3.  Abra o notebook [Nome do Notebook] no
