# -*- coding: utf-8 -*-
"""PipelineMulheresNaTecnologia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16a1AEhFoueWkiVsCsQ-IwJJ4RlaTMUS_
"""

!echo "Iniciando a limpeza profunda do ambiente..."

# 1. Remove o Data Warehouse e o Log no diret√≥rio raiz
!rm -f data_warehouse.db
!rm -f pipeline.log

# 2. Executa o comando de limpeza padr√£o do dbt no diret√≥rio do projeto
# Isso remove 'target' e 'dbt_packages' de forma segura.
!dbt clean --project-dir /content/pipeline_mulheres_na_tecnologia

# 3. Remove for√ßadamente os diret√≥rios do projeto dbt caso 'dbt clean' falhe
!rm -rf /content/pipeline_mulheres_na_tecnologia/target
!rm -rf /content/pipeline_mulheres_na_tecnologia/dbt_packages

# 4. Remove o banco de dados do dbt se ele foi criado dentro da pasta do projeto
!rm -f /content/pipeline_mulheres_na_tecnologia/data_warehouse.db

!echo "Limpeza conclu√≠da. Ambiente pronto para re-execu√ß√£o."

!pip install pandas prefect dbt-sqlite requests -q

import pandas as pd
import requests
import json
import sqlite3
print("Ambiente configurado com sucesso‚úÖ")

# Importando a base de dados - Pesquisa sobre mulheres na tecnologia do Kaggle

!wget -o kaggle_survey_2022.csv "https://raw.githubusercontent.com/paulalcssantos/Desafio-Pipeline-WoMakersCode/refs/heads/main/kaggle_survey_2022_mulheres_dados.csv"

import sqlite3

# Criando o banco SQL - Simulando participantes do nosso Bootcamp

conn_bootcamp = sqlite3.connect("bootcampBI.db")
cursor_bootcamp = conn_bootcamp.cursor()

cursor_bootcamp.execute('''
CREATE TABLE IF NOT EXISTS PARTICIPANTES(
  ID_PARTICIPANTE INT,
  NOME VARCHAR(150),
  PAIS_ORIGEM VARCHAR(100)
)
''')

participantes = [
    (1, 'Maria', 'Brazil'),
    (2, 'Luana', 'Portugal'),
    (3, 'Camila', 'Brazil'),
    (4, 'Luiza', 'Argentina'),
    (5, 'Silvia', 'Colombia'),
    (6, 'Paola', 'Brazil'),
    (7, 'Vitoria', 'Mexico'),
    (8, 'Caroline', 'Argentina'),
    (9, 'Marta', 'Portugal'),
    (10, 'Ana', 'Brazil')
]

cursor_bootcamp.executemany("INSERT INTO PARTICIPANTES VALUES (?, ?, ?)", participantes)
conn_bootcamp.commit()
conn_bootcamp.close()

print("Banco de dados e tabela criados com sucesso ‚úÖ")

# Commented out IPython magic to ensure Python compatibility.
# # Criando um arquivo JSON para agrupar linguagens usadas por categorias
# 
# %%writefile habilidades_categorias.json
# {
#   "Ferramentas de An√°lise": [
#     "Python",
#     "R",
#     "SQL"
#   ],
#   "Ferramentas de BI": [
#     "Power BI",
#     "Tableau",
#     "Looker"
#   ],
#   "Plataformas de Nuvem": [
#     "AWS",
#     "Google Cloud",
#     "Microsoft Azure"
#   ]
# }

# DATA WAREHOUSE

conn_datawarehouse = sqlite3.connect('data_warehouse.db')
conn_datawarehouse.close()

print("Data Warehouse criado com sucesso‚úÖ")

# Criando o arq. de logging
import logging

# Configurar o logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename='pipeline.log',
    filemode='w'
)

logger = logging.getLogger()

print("Logging configurado com sucesso‚úÖ")

# "Rob√¥" para extrair arq. CSV

import pandas as pd

def extrair_dados_kaggle(caminho_arquivo):
  try:
    logger.info(f"Iniciando a extra√ß√£o dos dados do arquivo: {caminho_arquivo}")
    df = pd.read_csv(caminho_arquivo)

    logger.info(f"Extra√ß√£o do CSV conclu√≠da. {len(df)} linhas lidas e {len(df.columns)}colunas.")
    logger.info(f"Tipos de dados das colunas: \n{df.dtypes}")

    return df

  except FileNotFoundError:
    logger.error(f"Arquivo n√£o encontrado: {caminho_arquivo}")
    return None
  except Exception as e:
      logger.error(f"Ocorreu um erro durante a extra√ß√£o dos dados: {e}")
      return None

# Teste da fun√ß√£o

df_kaggle = extrair_dados_kaggle("kaggle_survey_2022_mulheres_dados.csv")

if df_kaggle is not None:
  display(df_kaggle.head())

# "Rob√¥" que extrai banco de dados SQL

def extrair_dados_sql(caminho_banco):

    conexao = None
    try:

        logger.info(f"Iniciando a extra√ß√£o dos dados do banco: {caminho_banco}")

        conexao = sqlite3.connect(caminho_banco)

        query = "SELECT * FROM PARTICIPANTES"
        df = pd.read_sql_query(query, conexao)

        logger.info(f"Extra√ß√£o do banco de dados conclu√≠da com sucesso‚úÖ")
        return df

    except sqlite3.Error as e:
        logger.error(f"Ocorreu um erro ao conectar o banco de dados: {e}")
        return None
    except Exception as e:
        logger.error(f" Ocorreu um erro durante a extra√ß√£o dos dados: {e}")
        return None

    finally:
        if conexao:
            conexao.close()
            logger.info("Conex√£o com o banco de dados fechada.")

# Teste da fun√ß√£o

df_participantes = extrair_dados_sql("bootcampBI.db")

if df_participantes is not None:
    display(df_participantes.head())

# Carregar no Data Warehouse

def carregar_dados(df, nome_tabela, caminho_dw):
    if df is None:
      logger.warning("DataFrame vazio. Nenhum dado para carregar.")
      return

    try:
      logger.info(f"Iniciando o carregamento dos dados na tabela: {nome_tabela}")

      conexao_dw = sqlite3.connect(caminho_dw)

      df.to_sql(nome_tabela, conexao_dw, if_exists='replace', index=False)

      logger.info(f"Carregamento dos dados na tabela {nome_tabela} conclu√≠do com sucesso‚úÖ")

    except sqlite3.Error as e:
      logger.error(f"Ocorreu um erro ao carregar os dados para a tabela '{nome_tabela}': {e}")

    finally:
      if conexao_dw:
        conexao_dw.close()
        logger.info("Conex√£o com o Data Warehouse fechada.")

# Executa a carga dos DaraFrames extra√≠dos

carregar_dados(df_kaggle, "kaggle_survey", "data_warehouse.db")
carregar_dados(df_participantes, "Participantes", "data_warehouse.db")

# Extra√ß√£o API
# Vamos usar a biblioteca requests, que faz a chamada de API

import requests
import pandas as pd
import logging

logger = logging.getLogger()

def extrair_dados_paises_api(url_api):
  try:
    logger.info(f"Iniciando a requisi√ß√£o √† API: {url_api}")

    resposta = requests.get(url_api)

    if resposta.status_code == 200: # Se retornar 404 √© erro
      dados_json = resposta.json()
      logger.info(f"Dados da API extra√≠dos com sucesso‚úÖ. {len(dados_json)} registros de pa√≠ses.")
      return dados_json
    else:
      logger.error(f"Falha na requisi√ß√£o √† API. C√≥digo de status: {resposta.status_code}")
      return None

  except Exception as e:
    logger.error(f"Ocorreu um erro inesperado ao extrair os dados.")

url_paises = "https://restcountries.com/v3.1/all?fields=name,cca3,region"

# Teste da fun√ß√£o
dados_paises = extrair_dados_paises_api(url_paises)

if dados_paises is not None:
  df_paises = pd.json_normalize(dados_paises)
  display(df_paises.head())

# Ler o JSON

import json
import pandas as pd
import logging

logger = logging.getLogger()

def extrair_categorias_habilidades_json(caminho_arquivo):
  try:
    logger.info(f"Iniciando a extra√ß√£o dos arquivo JSON: {caminho_arquivo}")

    with open(caminho_arquivo, 'r') as arquivo: # transforma em uma lista de dicion√°rio do python
      dados_json = json.load(arquivo)

    logger.info("Extra√ß√£o do json conclu√≠da com sucesso‚úÖ")
    return dados_json

  except FileNotFoundError:
    logger.error(f"Arquivo n√£o encontrado: {caminho_arquivo}")
    return None
  except Exception as e:
    logger.error(f"Ocorreu um erro inesperado durante a extra√ß√£o dos dados: {e}")
    return None

# Teste da fun√ß√£o
dados_habilidades = extrair_categorias_habilidades_json("habilidades_categorias.json")

if dados_habilidades is not None:
  df_habilidades = pd.json_normalize(dados_habilidades)
  display(df_habilidades.head())

# Precisamos dos dados separados - JSON

def transformar_json_em_df(dados_json):
  if dados_json is None:
    logger.warning("Dados JSON vazios. Nenhum dado para transformar.")
    return None

  lista_categorias = []

  for categoria, habilidades in dados_json.items():
    for habilidade in habilidades:
      lista_categorias.append({"categoria": categoria,"habilidade": habilidade})

  df_categorias = pd.DataFrame(lista_categorias)

  logger.info(f"Transforma√ß√£o do JSON em dataframe conclu√≠da com sucesso!")
  return df_categorias

# Teste
df_categorias = transformar_json_em_df(dados_habilidades)
if df_categorias is not None:
  display(df_categorias.head())

# Carregando na DW

carregar_dados(df_paises, "paises", "data_warehouse.db")
carregar_dados(df_categorias, "categorias", "data_warehouse.db")

# Transforma√ß√£o com DBT - ling. SQL
!dbt init pipeline_mulheres_na_tecnologia

import yaml
import os

# Criando script para subscrever profiles (arq. ocultos)
# √â um dicion√°rio que traz os logs de senha e afins
profiles_config = {
    'pipeline_mulheres_na_tecnologia': {
        'target': 'dev',
        'outputs': {
            'dev': {
                'type': 'sqlite',
                'threads': 1, #quant. de tarefas em paralelo

                # Par√¢metro para vers√µes mais recentes
                'schemas_and_paths': {
                    'main': '../data_warehouse.db'
                },

                # Par√¢metro para vers√µes mais antigas
                'database': '../data_warehouse.db',

                # Par√¢metro para vers√µes mais antigas
                'schema': 'main',

                # Par√¢metro opcional, mas boa pr√°tica
                'schema_directory': '.',
            }
        }
    }
}

dbt_profiles_dir = os.path.expanduser('~/.dbt')
os.makedirs(dbt_profiles_dir, exist_ok=True)
profiles_path = os.path.join(dbt_profiles_dir, 'profiles.yml')

with open(profiles_path,'w') as f:
    yaml.dump(profiles_config, f)

# Commented out IPython magic to ensure Python compatibility.
# Ver se a conec√ß√£o com o dbt est√° funcionando
# %cd pipeline_mulheres_na_tecnologia
!dbt debug

# Commented out IPython magic to ensure Python compatibility.
# Antes de tudo, ir em pipeline (..) models > criar pasta stg > criar arq. "source.yml"

# stg com dbt

# %cd pipeline_mulheres_na_tecnologia
!dbt run

import os
os.makedirs("/content/pipeline_mulheres_na_tecnologia/models/staging", exist_ok=True)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/pipeline_mulheres_na_tecnologia/models/staging/source.yml
# version: 2
# 
# sources:
#   - name: main # Alterado para corresponder ao alias em profiles.yml
#     # Removendo 'database' e 'schema' aqui para evitar redund√¢ncia
# 
#     tables:
#       - name: kaggle_survey
#         description: "Dados brutos da pesquisa do kaggle, filtrado por mulheres na √°rea de dados"
#       - name: participantes
#         description: "Dados das participantes do bootcamp"
#       - name: paises
#         description: "Dados dos pa√≠ses extra√≠dos da API REST countries"
#       - name: categorias
#         description: "Mapeamento das habilidades e categorias, extra√≠do do arquivo JSON"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/pipeline_mulheres_na_tecnologia/models/staging/stg_kaggle_survey.sql
# 
# SELECT PAIS,
#        NIVEL_EDUCACIONAL,
#        ANOS_PROGRAMANDO,
#        CARGO_ATUAL,
#        ANOS_USANDO_ML,
#        CASE WHEN SALARIO_ANUAL_USD LIKE '%-%'
#             THEN CAST(REPLACE(SUBSTR(SALARIO_ANUAL_USD, 1, INSTR(SALARIO_ANUAL_USD, '-')-1),',','') AS REAL)
#             WHEN SALARIO_ANUAL_USD IS NOT NULL
#             THEN CAST(REPLACE(SALARIO_ANUAL_USD,',','') AS REAL)
#             ELSE NULL
#       END AS SALARIO_ANUAL_USD
# FROM {{source('main','kaggle_survey')}}

!dbt run

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/pipeline_mulheres_na_tecnologia/models/staging/stg_participantes.sql
# SELECT ID_PARTICIPANTE,
#        NOME,
#        PAIS_ORIGEM
# FROM {{source('main','participantes')}}

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/pipeline_mulheres_na_tecnologia/models/staging/stg_paises.sql
# SELECT name_common AS NOME_PAIS,
#         cca3         AS CODIGO_PAIS,
#         region       AS REGIAO
# FROM {{source('main','paises')}}

import os
os.makedirs("/content/pipeline_mulheres_na_tecnologia/models/marts", exist_ok=True)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/pipeline_mulheres_na_tecnologia/models/marts/dim_desenvolvedoras.sql
# {{ config(materialized = 'table') }}
# 
# WITH stg_kaggle AS (
#   SELECT * FROM {{ref('stg_kaggle_survey')}}
# ),
# stg_paises AS (
#   SELECT * FROM {{ref('stg_paises')}}
# )
# 
# SELECT
#   stg_kaggle.PAIS,
#   stg_kaggle.NIVEL_EDUCACIONAL,
#   stg_kaggle.ANOS_PROGRAMANDO,
#   stg_kaggle.CARGO_ATUAL,
#   stg_kaggle.ANOS_USANDO_ML,
#   stg_kaggle.SALARIO_ANUAL_USD
#   -- stg_kaggle.LINGUAGENS_USADAS, -- Removido temporariamente
#   -- stg_kaggle.BANCO_DE_DADOS_USADOS, -- Removido temporariamente
#   -- stg_kaggle.FERRAMENTAS_BI_USADAS, -- Removido temporariamente
#   stg_paises.CODIGO_PAIS,
#   stg_paises.REGIAO
# FROM stg_kaggle
# LEFT JOIN stg_paises ON stg_kaggle.PAIS = stg_paises.NOME_PAIS

import os
os.makedirs('/content/pipeline_mulheres_na_tecnologia/models/marts', exist_ok=True)

!dbt run

!dbt run --debug

# ------------------------------------------------------------------------------
# CRIA√á√ÉO AUTOM√ÅTICA DO AMBIENTE (DW, JSON, DBT PROJETO)
# ------------------------------------------------------------------------------

# Data Warehouse
sqlite3.connect("data_warehouse.db").close()

# Criar pasta do dbt se n√£o existir
!mkdir -p /content/pipeline_mulheres_na_tecnologia/models/staging
!mkdir -p /content/pipeline_mulheres_na_tecnologia/models/marts

# Criar profiles.yml corretamente
# ... (Seu c√≥digo profiles.yml original) ...
print("profiles.yml criado com sucesso!")

# ----------------------------------------------------------------
# <<<<<<<<< INSIRA AQUI O C√ìDIGO DO dbt_project.yml >>>>>>>>>>
# ----------------------------------------------------------------
# Criar dbt_project.yml (CORRE√á√ÉO ESSENCIAL)
dbt_project_config = {
    'name': 'pipeline_mulheres_na_tecnologia',
    'config-version': 2,
    'profile': 'pipeline_mulheres_na_tecnologia',
    'model-paths': ['models'],
    'seed-paths': ['seeds'],
    'macro-paths': ['macros'],
    'snapshot-paths': ['snapshots'],
    'target-path': 'target',
    'clean-targets': ['target', 'dbt_packages', 'dbt_modules'],
}

project_path = "/content/pipeline_mulheres_na_tecnologia"
with open(os.path.join(project_path, 'dbt_project.yml'), 'w') as f:
    yaml.dump(dbt_project_config, f)

print("dbt_project.yml criado com sucesso!")

# ==============================================================================
# C√âLULA COMPLETA ‚Äî PIPELINE ETL ORQUESTRADO (Prefect + dbt + SQLite)
# ==============================================================================

# 1. Instala√ß√µes
!pip install pandas prefect dbt-sqlite requests pyyaml -q

# 2. Imports
import pandas as pd
import requests
import json
import sqlite3
from prefect import task, flow, get_run_logger
import subprocess
import os
import logging
import yaml

# Definir vari√°veis de ambiente para o Prefect
os.environ["PREFECT_API_URL"] = "http://127.0.0.1:4200"
os.environ["PREFECT_UI_URL"] = "http://127.0.0.1:4200"

# ------------------------------------------------------------------------------
# LOGGING
# ------------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename='pipeline.log',
    filemode='w'
)
logger = logging.getLogger()
print("Logging configurado com sucesso!")

# ------------------------------------------------------------------------------
# CRIA√á√ÉO AUTOM√ÅTICA DO AMBIENTE (DW, JSON, DBT PROJETO)
# ------------------------------------------------------------------------------

# Data Warehouse
sqlite3.connect("data_warehouse.db").close()

# Criar pasta do dbt se n√£o existir
!mkdir -p /content/pipeline_mulheres_na_tecnologia/models/staging
!mkdir -p /content/pipeline_mulheres_na_tecnologia/models/marts

# Criar profiles.yml corretamente
profiles_config = {
    'pipeline_mulheres_na_tecnologia': {
        'target': 'dev',
        'outputs': {
            'dev': {
                'type': 'sqlite',
                'threads': 1,
                'database': '../data_warehouse.db',
                'schemas_and_paths': {
                    'main': '../data_warehouse.db'
                },
                'schema': 'main',
                'schema_directory': '.',
            }
        }
    }
}

dbt_profiles_dir = os.path.expanduser('~/.dbt')
os.makedirs(dbt_profiles_dir, exist_ok=True)

with open(os.path.join(dbt_profiles_dir, 'profiles.yml'), 'w') as f:
    yaml.dump(profiles_config, f)

print("profiles.yml criado com sucesso!")

# ------------------------------------------------------------------------------
# TAREFAS (E L T)
# ------------------------------------------------------------------------------

# =======================   EXTRA√á√ÉO CSV   ================================
@task(retries=3, retry_delay_seconds=5)
def extrair_dados_kaggle(caminho_arquivo):
    try:
        df = pd.read_csv(caminho_arquivo)
        logger.info(f"CSV extra√≠do com {len(df)} linhas.")
        return df
    except Exception as e:
        logger.error(f"Erro ao extrair CSV: {e}")
        return None

# =======================   EXTRA√á√ÉO SQL   ================================
@task
def extrair_dados_sql(caminho_banco):
    try:
        conexao = sqlite3.connect(caminho_banco)
        df = pd.read_sql_query("SELECT * FROM PARTICIPANTES", conexao)
        conexao.close()
        logger.info("SQL extra√≠do com sucesso.")
        return df
    except Exception as e:
        logger.error(f"Erro SQL: {e}")
        return None

# =======================   EXTRA√á√ÉO API   ================================
@task
def extrair_dados_paises_api(url_api):
    try:
        resposta = requests.get(url_api)
        dados = resposta.json() if resposta.status_code == 200 else None
        logger.info(f"API retornou {len(dados)} pa√≠ses.")
        return dados
    except Exception as e:
        logger.error(f"Erro API: {e}")
        return None

# =======================   EXTRA√á√ÉO JSON   ================================
@task
def extrair_habilidades_json(caminho_arquivo):
    try:
        with open(caminho_arquivo, 'r') as f:
            dados = json.load(f)
        logger.info("JSON extra√≠do.")
        return dados
    except Exception as e:
        logger.error(f"Erro JSON: {e}")
        return None

# =======================   TRANSFORMA√á√ÉO JSON   ==========================
@task
def transformar_json(dados_json):
  if dados_json is None:
    logger.warning("Dados JSON vazios. Nenhum dado para transformar.")
    return None

  lista_categorias = []

  for categoria, habilidades in dados_json.items():
    for habilidade in habilidades:
      lista_categorias.append({"categoria": categoria,"habilidade": habilidade})

  df_categorias = pd.DataFrame(lista_categorias)

  logger.info(f"Transforma√ß√£o do JSON em dataframe conclu√≠da com sucesso!")
  return df_categorias

# =======================   CARGA NO DW   =================================
@task
def carregar_no_dw(df, tabela):
    if df is None:
        logger.warning(f"{tabela} vazio.")
        return

    try:
        conn = sqlite3.connect("data_warehouse.db")
        cursor = conn.cursor()
        # Remover tabela antes de recriar (garante limpeza)
        cursor.execute(f"DROP TABLE IF EXISTS {tabela}")

        df.to_sql(tabela, conn, if_exists="replace", index=False)
        conn.close()
        logger.info(f"Tabela {tabela} carregada no DW.")
    except Exception as e:
        logger.error(f"Erro carga {tabela}: {e}")

# =======================   EXECU√á√ÉO DBT   ================================
@task
def executar_dbt_run():
    logger = get_run_logger()
    try:
        # Capturar stdout e stderr do subprocesso dbt
        result = subprocess.run(["dbt", "run"], check=True, cwd="/content/pipeline_mulheres_na_tecnologia", capture_output=True, text=True)
        logger.info(f"dbt run executado com sucesso!\nSTDOUT:\n{result.stdout}\nSTDERR:\n{result.stderr}")
    except subprocess.CalledProcessError as e:
        logger.error(f"Erro no dbt run: {e}\nSTDOUT:\n{e.stdout}\nSTDERR:\n{e.stderr}")
        raise

# ------------------------------------------------------------------------------
# FLOW PRINCIPAL
# ------------------------------------------------------------------------------
@flow(name="Pipeline ETL - Mulheres na Tecnologia")
def pipeline_principal():
    logger = get_run_logger()
    logger.info("üöÄ Iniciando Pipeline ETL")

    # ====================== EXTRA√á√ÉO ==========================
    df_kaggle = extrair_dados_kaggle("/content/kaggle_survey_2022.csv")
    df_participantes = extrair_dados_sql("/content/bootcampBI.db")
    df_paises = extrair_dados_paises_api("https://restcountries.com/v3.1/all?fields=name,cca3,region")
    habilidades_json = extrair_habilidades_json("/content/habilidades_categorias.json")

    # ====================== TRANSFORMA√á√ÉO =====================
    df_paises_df = pd.json_normalize(df_paises)
    # Renomear a coluna 'name.common' para 'name_common' para evitar problemas com SQLite
    if 'name.common' in df_paises_df.columns:
        df_paises_df = df_paises_df.rename(columns={'name.common': 'name_common'})

    df_habilidades = transformar_json(habilidades_json)

    # ====================== CARGA =============================
    carregar_no_dw(df_kaggle, "kaggle_survey")
    carregar_no_dw(df_participantes, "participantes")
    carregar_no_dw(df_paises_df, "paises")
    carregar_no_dw(df_habilidades, "categorias")

    # ====================== TRANSFORMA√á√ÉO DBT =================
    executar_dbt_run()

    logger.info("üéâ Pipeline ETL finalizado com sucesso!")


# ------------------------------------------------------------------------------
# EXECUTAR O PIPELINE
# ------------------------------------------------------------------------------
if __name__ == "__main__":
   pipeline_principal()

